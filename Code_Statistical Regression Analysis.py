# -*- coding: utf-8 -*-
"""CA2.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1R_PfMFT7uSKJA-0m1mlU1b-VkmQoXLhd
"""

# Loading the libraries and functions
from pandas import read_csv, DataFrame,get_dummies
from plotly import figure_factory
from sklearn.preprocessing import StandardScaler
from sklearn import linear_model
from sklearn.model_selection import GridSearchCV

"""## ***Data Preparation***"""

# Loading the dataset
Store=read_csv('/content/Store_CA.csv')

# Reading the file from start
Store.head()

# Statistical characteristics of the numerical columns in the dataset
Store.describe()

# Summary of dataset structure
Store.info()

#We won't be dropping any column as they all are directly proportional to our Target variable

# identify any missing (null) values
Store.isnull().any()

# Retrieve the unique values for all columns
Store['StoreLocation'].unique()

Store['StoreLocation']=Store['StoreLocation'].map({'Los Angeles':0,'Sacramento':1,'Palo Alto':2,'San Francisco':3})

Store['StoreCategory'].unique()

Store['StoreCategory']=Store['StoreCategory'].map({'Electronics':0,'Grocery':1,'Clothing':2})

#Retrieve the dimensions
Store.shape

# Checking for Outliers using IQR
Q1 = Store.quantile(0.25)
Q3 = Store.quantile(0.75)
IQR = Q3 - Q1
Store = Store[~((Store < (Q1 - 1.5 * IQR)) | (Store > (Q3 + 1.5 * IQR))).any(axis=1)]
print(f"Dataset Shape After Removing Outliers: {Store.shape}")

# Summary of dataset structure(after transformation)
Store.info()# Checking the updated dataset

# Correlation between features/variables based on heat map
cor=Store.corr() # Correlation matrix
f=figure_factory.create_annotated_heatmap(cor.values,list(cor.columns),list(cor.columns),cor.round(2).values,showscale=True)
f.show()

# Seggregate the dataset into dependent and independent variables
X = Store.drop(['MonthlySalesRevenue'],axis=1)
Y = Store['MonthlySalesRevenue']

# Standardisation/normalisation of the dataset
X_st=StandardScaler().fit_transform(X)

"""Linear Regression for predicting MonthlySalesRevenue"""

# Linear Regression
LR=linear_model.SGDRegressor(random_state=10,penalty=None) # Linear regression without the regularisation
hyper_param={'eta0':[0.005,0.003,0.04,0.02,0.25,0.1,1],'max_iter':[1000,1100,1300,1500,1800,2000]}
grid_search=GridSearchCV(estimator=LR,param_grid=hyper_param,scoring='r2',cv=7) # R^2 tells how the good the linear model has fitted the dataset.
grid_search.fit(X_st,Y) # Fitting the linear model

LR.fit(X_st,Y) # Fitting the Linear model without penalty
LR.score(X_st,Y) # R^2 score tells how much variation in the dependent variable is explained by independent variable, i.e., 76% of variation in Price is explained by independent variables in the dataset.

# Optimal parameters
best_params= grid_search.best_params_
print(best_params)
best_result=grid_search.best_score_
print(best_result)
best_model=grid_search.best_estimator_
print('Beta_0:',best_model.intercept_)
#best_model.coef_
print(DataFrame(zip(X.columns,best_model.coef_),columns=['Columns/features','Beta coefficients']))

"""## ***Regularisation***"""

# Adding the penalty in the model with penalty = 'elasticnet'
LR1=linear_model.SGDRegressor(random_state=10,penalty='elasticnet') # Linear regression without the regularisation
hyper_param={'eta0':[0.005,0.003,0.04,0.02,0.25,0.1,1],'max_iter':[1000,1100,1300,1500,1800,2000],'alpha':[0.001,0.01,0.1,1],'l1_ratio':[0,0.25,0.3,0.4,0.5,0.75,1]}
grid_search=GridSearchCV(estimator=LR1,param_grid=hyper_param,scoring='r2',cv=7) # R^2 tells how the good the linear model has fitted the dataset.
grid_search.fit(X_st,Y) # Fitting the linear model

grid_search.score(X_st,Y)

# Optimal parameters
best_params= grid_search.best_params_
print(best_params)
best_result=grid_search.best_score_
print(best_result)
best_model=grid_search.best_estimator_
print('Beta_0:',best_model.intercept_)
#best_model.coef_
print(DataFrame(zip(X.columns,best_model.coef_),columns=['Columns/features','Beta coefficients']))

# Adding the penalty in the model with penalty = 'l1'
LR2=linear_model.SGDRegressor(random_state=10,penalty='l1') # Linear regression without the regularisation
hyper_param1={'eta0':[0.005,0.003,0.04,0.02,0.25,0.1,1],'max_iter':[1000,1100,1300,1500,1800,2000],'alpha':[0.001,0.01,0.1,1]}
grid_search1=GridSearchCV(estimator=LR2,param_grid=hyper_param1,scoring='r2',cv=7) # R^2 tells how the good the linear model has fitted the dataset.
grid_search1.fit(X_st,Y) # Fitting the linear model

grid_search1.score(X_st,Y)

# Optimal parameters
best_params1= grid_search1.best_params_
print(best_params1)
best_result1=grid_search1.best_score_
print(best_result1)
best_model1=grid_search1.best_estimator_
print('Beta_0:',best_model1.intercept_)
#best_model.coef_
print(DataFrame(zip(X.columns,best_model1.coef_),columns=['Columns/features','Beta coefficients']))

# Adding the penalty in the model with penalty = 'l2'
LR3=linear_model.SGDRegressor(random_state=10,penalty='l2') # Linear regression without the regularisation
hyper_param2={'eta0':[0.005,0.003,0.04,0.02,0.25,0.1,1],'max_iter':[1000,1100,1300,1500,1800,2000],'alpha':[0.001,0.01,0.1,1],'l1_ratio': [0]}
grid_search2=GridSearchCV(estimator=LR3,param_grid=hyper_param2,scoring='r2',cv=7) # R^2 tells how the good the linear model has fitted the dataset.
grid_search2.fit(X_st,Y) # Fitting the linear model

grid_search2.score(X_st,Y)

# Optimal parameters
best_params2= grid_search2.best_params_
print(best_params2)
best_result2=grid_search2.best_score_
print(best_result2)
best_model2=grid_search2.best_estimator_
print('Beta_0:',best_model2.intercept_)
#best_model.coef_
print(DataFrame(zip(X.columns,best_model2.coef_),columns=['Columns/features','Beta coefficients']))

LR2=linear_model.SGDRegressor(random_state=1,penalty='elasticnet',alpha= 1, eta0= 0.003, l1_ratio= 1, max_iter=1000) # Fitting the Linear mod

LR2.fit(X_st,Y)# Fitting the Linear model with penalty elasticnet and using the bestparameters from GridsearchCV
LR2.score(X_st,Y) # R^2 score tells how much variation in the dependent variable is explained by independent variable, i.e., 80% of variation in MonthlySalesRevenue is explained by independent variables in the dataset.

LR2.predict(X[:10]) # Prediction of first 10 rows in the dataset

X[:10]

"""Support Vector Regressor"""

from pandas import  read_csv, DataFrame,Series
from sklearn.preprocessing import StandardScaler
from sklearn.model_selection import GridSearchCV
#from sklearn import linear_model
from sklearn.linear_model import SGDRegressor
from plotly import graph_objs, figure_factory

# 1st way with L2 regression parameter .i.e., considering 'C'

#Support Vector Regression
from sklearn.svm import SVR
SVRegressor = SVR()
cv=10
Hparameters = {'kernel': ['linear', 'poly', 'rbf', 'sigmoid'], 'C': [1000,10000,100000], 'epsilon': [100,1000,10000]}
grid_search1 = GridSearchCV(estimator=SVRegressor, param_grid=Hparameters, scoring='r2')
grid_search1.fit(X, Y)

best_parameters = grid_search1.best_params_
print("Best parameters: ", best_parameters)
best_result = grid_search1.best_score_
print("Best result: ", best_result)

svr_regressor2= SVR(kernel='linear',epsilon=100)
svr_regressor2.fit(X, Y)
svr_regressor2.score(X, Y)

svr_regressor2.score(X, Y)

#Modified mean square error, is used if the Best Result is in negative.

#Support Vector Regression without C parameter
from sklearn.svm import SVR
SVRegressor = SVR()
cv=10
Hparameters = {'kernel': ['linear', 'poly', 'rbf', 'sigmoid'], 'epsilon': [100,1000,10000]} #Without L2 regression Parameter
grid_search2 = GridSearchCV(estimator=SVRegressor, param_grid=Hparameters, scoring='r2')
grid_search2.fit(X, Y)

best_parameters = grid_search2.best_params_
print("Best parameters: ", best_parameters)
best_result = grid_search2.best_score_
print("Best result: ", best_result)

svr_regressor= SVR(kernel='poly', C=10000, epsilon=1000)
svr_regressor.fit(X, Y)

svr_regressor2.coef_
DataFrame(zip(X.columns,svr_regressor2.coef_),columns=['Columns/features','Beta coefficients'])

"""Random Forest Regressor"""

from sklearn.ensemble import RandomForestRegressor
RF_Regressor1 = RandomForestRegressor(criterion='squared_error', max_features='sqrt', random_state=1)
no_Trees = {'n_estimators': [15,30,45,60,75,90,105], 'max_features':['sqrt','log2', None]}
grid_search3 = GridSearchCV(estimator=RF_Regressor1, param_grid=no_Trees, scoring='r2', cv=10)
grid_search3.fit(X, Y)

best_parameters = grid_search3.best_params_
print("Best parameters: ", best_parameters)
best_result = grid_search3.best_score_
print("best_score: ", best_result)

from sklearn import ensemble

# Refit the random forest on the optimal n_estimators parameter
RF2_regressor=ensemble.RandomForestRegressor(n_estimators=24,criterion='squared_error',max_features=None,random_state=1)
RF2_regressor.fit(X,Y)

best_result = RF2_regressor.score(X,Y)
print("r2: ", best_result)

from sklearn.ensemble import RandomForestRegressor
RF_Regressor2 = RandomForestRegressor(criterion='squared_error', max_features='sqrt', random_state=1)
no_Trees = {'n_estimators': [10, 15, 30, 40, 60, 75, 105], 'max_features':['sqrt','log2', None]}
grid_search4 = GridSearchCV(estimator=RF_Regressor2, param_grid=no_Trees, scoring='r2', cv=10)
grid_search4.fit(X, Y)

best_parameters = grid_search4.best_params_
print("Best parameters: ", best_parameters)
best_result = grid_search4.best_score_
print("best_score: ", best_result)
Important_feature = Series(grid_search4.best_estimator_.feature_importances_, index=list(X)).sort_values(ascending=False) # Getting feature importances list for the best model
print(Important_feature)

"""Features with higher value are significant, hence redefining the model with these significant features"""

X = Store[['ProductVariety', 'StoreSize', 'EmployeeEfficiency', 'CustomerFootfall' , 'EconomicIndicator']]
X= StandardScaler().fit_transform(X)

from sklearn.ensemble import RandomForestRegressor
RF_Regressor2 = RandomForestRegressor(criterion='squared_error', max_features='sqrt', random_state=1)
no_Trees = {'n_estimators': [10, 15, 30, 40, 60, 75, 105], 'max_features':['sqrt','log2', None]}
grid_search5 = GridSearchCV(estimator=RF_Regressor2, param_grid=no_Trees, scoring='r2', cv=10)
grid_search5.fit(X, Y)

best_parameters = grid_search5.best_params_
print("Best parameters: ", best_parameters)
best_result = grid_search.best_score_

# Refit the random forest on the optimal n_estimators parameter
RF_regressor_feature=ensemble.RandomForestRegressor(n_estimators=12,criterion='squared_error',max_features=None,random_state=1)
RF_regressor_feature.fit(X, Y)

best_result = RF_regressor_feature.score(X, Y)
print("r2: ", best_result)

# Prediction with New Data
import pandas as pd
new_data = pd.DataFrame({
    'StoreSize': [1500],
    'CustomerFootfall': [2000],
    'StoreAge': [10],
    'EmployeeEfficiency': [85],
    'EconomicIndicator': [1.2],
    'StoreLocation': [1],  # 'Sacramento' as StoreLocation
    'StoreCategory': [0]   # 'Electronics' as StoreCategory
})

# Get the original column names from before the transformation
original_columns = ['ProductVariety', 'StoreSize', 'EmployeeEfficiency', 'CustomerFootfall' , 'EconomicIndicator']

for col in original_columns:
    if col not in new_data.columns:
        new_data[col] = 0  # to handle missing columns

new_data = new_data[original_columns]

# Feature scaling
from sklearn.preprocessing import StandardScaler
scaler = StandardScaler()
scaler.fit(new_data)
new_data_scaled = scaler.transform(new_data)

# Prediction
prediction = RF_regressor_feature.predict(new_data_scaled)  # Assuming RF_regressor_feature is the intended model
print("Prediction for New Data (Random Forest):", prediction)