# -*- coding: utf-8 -*-
"""CA ONE

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1kQAk5kQGzzdmUpHlUCmUZqR6Z3HQTvT7

Data Preparation
"""

# Loading the libraries
from pandas import read_csv, get_dummies, DataFrame, Series
from sklearn.preprocessing import StandardScaler
from sklearn.model_selection import train_test_split
from imblearn.over_sampling import SMOTE

data= read_csv('/content/bank.csv')

data.shape    # Dimensions of the dataset

data.info()

data.describe() # Descriptive Statistics for the numerical variable

data['default'].unique()

data['housing'].unique()

data['loan'].unique()

data['y'].unique()

data['job'].unique()

data['marital'].unique()

data['education'].unique()

data['contact'].unique()

data['month'].unique()

data['poutcome'].unique()

data['y'].value_counts()

data['y'].value_counts(normalize=True)

data['default']= data['default'].map({'yes':1,'no':0}) # Converting default column into numeric values i.e., 0 and 1.
#data.head
#default
#housing

data['housing']= data['housing'].map({'yes':1, 'no':0})
#data.head

data['y']= data['y'].map({'yes':1,'no':0})

data['loan']= data['loan'].map({'yes':1, 'no':0})

data.info()

features=['job' , 'marital' , 'education' , 'contact' , 'month' , 'poutcome']
data1=get_dummies(data,columns=features)
data1.head

data1.info()

data1.shape

y = data1['y'].map({'yes': 1, 'no': 0})

Y=data1['y'] # Dependent variable

X=data1.drop('y',axis=1) # Independent Variable

X.shape

Y.shape

X_scaled=StandardScaler().fit_transform(X) # Performing standardisation on the independent variables

DataFrame(X_scaled)

#X1=DataFrame(X_scaled)
#X1.describe()

X_train, X_test, Y_train, Y_test= train_test_split(X_scaled,Y, test_size=0.3, random_state=42)

X_train.shape

X_test.shape

Y_train.value_counts()

import pandas as pd
from imblearn.over_sampling import SMOTE

# Assuming Y_train is a Pandas Series
# Remove rows with NaN values from both X_train and Y_train
# This will remove data points where we don't have a valid target
X_train = X_train[Y_train.notna()]
Y_train = Y_train[Y_train.notna()]

smote=SMOTE(random_state=42)

X_train, Y_train= smote.fit_resample(X_train, Y_train)

Y_train.value_counts()

"""Random Forest Classifier

"""

from sklearn import ensemble
X_train = pd.DataFrame(X_train, columns=X.columns)
RF= ensemble.RandomForestClassifier(n_estimators=50,criterion='entropy',max_features='log2',random_state=42)
RF.fit(X_train,Y_train) # fitting random forest to training data
Y_pred=RF.predict(X_test)

# Evaluating the random forest
from sklearn import metrics
Accuracy= metrics.accuracy_score(Y_test,Y_pred) # Calculating the accuracy of the model build
Recall= metrics.recall_score(Y_test,Y_pred) # Calculating the recall of the model build
Precision= metrics.precision_score(Y_test,Y_pred) # Calculating the precision of the model build
print('Accuracy:', Accuracy)
print('Recall:', Recall)
print('Precision:', Precision)

from sklearn.model_selection import GridSearchCV

# Selecting the optimal n_estimators parameter
RF1=ensemble.RandomForestClassifier(n_estimators=50,criterion='entropy',max_features=None,random_state=42)
ntrees={'n_estimators': [50,100,150,180,220,250,300,320,350]} # Grid of n_estimators parameter length = 8

grid_search=GridSearchCV(estimator=RF1,param_grid=ntrees,scoring='precision',cv=5)

grid_search.fit(X_train,Y_train) # Fitting decision tree classifier to the training set
best_nestimator= grid_search.best_params_ # Selecting the optimal parameter n_estimators
print(best_nestimator)

# Refit the random forest on the optimal n_estimators parameter
RF2=ensemble.RandomForestClassifier(n_estimators=100,criterion='entropy',max_features=None,random_state=42)
RF2.fit(X_train,Y_train) # fitting random forest to training data
Y_pred=RF2.predict(X_test)

# Evaluating the refitted random forest with the optimal n_estimator parameter
from sklearn import metrics
Accuracy= metrics.accuracy_score(Y_test,Y_pred) # Calculating the accuracy of the model build
Recall= metrics.recall_score(Y_test,Y_pred) # Calculating the recall of the model build
Precision= metrics.precision_score(Y_test,Y_pred) # Calculating the precision of the model build
print('Accuracy:', Accuracy)
print('Recall:', Recall)
print('Precision:', Precision)

# Selecting the optimal features
rf_classifier = ensemble.RandomForestClassifier(n_estimators=100, criterion='entropy', random_state=42)
rf_classifier.fit(X_train, Y_train)
feature_importances = rf_classifier.feature_importances_
imp_features = Series(feature_importances, index=X_train.columns).sort_values(ascending=False)
print(imp_features)

"""Support Vector Classifier

"""

from sklearn import svm # Loading library of svm
from sklearn.svm import SVC # Loading function SVC from svm library
svm_classifier= SVC(kernel='linear',random_state=42) # Building classifier
svm_classifier.fit(X_train,Y_train) # Building classifier on the train X and Y train values

Y_pred= svm_classifier.predict(X_test) # Predciting the test values based on the SVM classifier
#Y_pred

# Evaulate the SVM classifier
from sklearn import metrics
Accuracy= metrics.accuracy_score(Y_test,Y_pred)
Precision=metrics.precision_score(Y_test, Y_pred)
Recall = metrics.recall_score(Y_test,Y_pred)
F1= metrics.f1_score(Y_test,Y_pred)

print('Accuracy:', Accuracy)
print('Precision:', Precision)
print('Recall:', Recall)
print('F1 score:', F1)

"""Support Vector Machine with Gridsearch"""

from imblearn.pipeline import Pipeline
from sklearn.model_selection import GridSearchCV
model=Pipeline([('balancing',SMOTE(random_state=42)),('classification', SVC(random_state=42))])

ker_c={'classification__kernel':['linear','poly','rbf','sigmoid'],'classification__C':[0.001,0.01,0.1,1,10]}

grid_search1=GridSearchCV(estimator=model,param_grid=ker_c, scoring='recall',cv=5)


grid_search1.fit(X_train,Y_train)

best_params=grid_search1.best_params_
best_params
best_res= grid_search1.best_score_
#print(best_res)

best_params

from sklearn import svm # Loading library of svm
from sklearn.svm import SVC # Loading function SVC from svm library
svm_classifier_rbf= SVC(kernel='poly',C=10,random_state=42) # Building classifier
svm_classifier_rbf.fit(X_train,Y_train) # Building classifier on the train X and Y train values

# Predicting the test values using the classifier
Y_pred=svm_classifier_rbf.predict(X_test)

# Evaluating the model based on chosen kernel
from sklearn import metrics

Accuracy= metrics.accuracy_score(Y_test,Y_pred)
Precision=metrics.precision_score(Y_test, Y_pred)
Recall = metrics.recall_score(Y_test,Y_pred)
F1= metrics.f1_score(Y_test,Y_pred)

print('Accuracy:', Accuracy)
print('Precision:', Precision)
print('Recall:', Recall)
print('F1 score:', F1)